{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KseniyaShilova/rl-game/blob/main/rlGameVideo124.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TmrznGhOEiEz"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import torch\n",
        "#from gym import make\n",
        "from torch import nn\n",
        "from torch.optim import Adam, lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h9qsCDLlEiHs"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.96\n",
        "INITIAL_STEPS = 1024\n",
        "TRANSITIONS = 4_300\n",
        "STEPS_PER_UPDATE = 1000\n",
        "STEPS_PER_TARGET_UPDATE = STEPS_PER_UPDATE * 1000\n",
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 100_000\n",
        "LEARNING_RATE =  0.000001\n",
        "DEVICE = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBaVU0ZeXrSr",
        "outputId": "df19c8b9-e27f-4d8c-b0ee-6d39245aa195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Arcade-Learning-Environment'...\n",
            "remote: Enumerating objects: 9443, done.\u001b[K\n",
            "remote: Counting objects: 100% (1238/1238), done.\u001b[K\n",
            "remote: Compressing objects: 100% (536/536), done.\u001b[K\n",
            "remote: Total 9443 (delta 790), reused 1010 (delta 671), pack-reused 8205\u001b[K\n",
            "Receiving objects: 100% (9443/9443), 7.69 MiB | 14.24 MiB/s, done.\n",
            "Resolving deltas: 100% (7263/7263), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Farama-Foundation/Arcade-Learning-Environment/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VTBvHoMJEiKK"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/Farama-Foundation/gym-examples\n",
        "# cd gym-examples\n",
        "\n",
        "# source .env/bin/activate\n",
        "#!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rptXPM5EWGft"
      },
      "outputs": [],
      "source": [
        "# from gym.envs.registration import register\n",
        "\n",
        "# register(\n",
        "#     id=\"gym_examples/GridWorld-v0\",\n",
        "#     entry_point=\"gym_examples.envs:GridWorldEnv\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIvyWY8efSth",
        "outputId": "661ff0e9-d885-4dd1-aeac-484d229c89bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[atari]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.1.1)\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy\n",
            "Successfully installed ale-py-0.8.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[atari]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlIMAeHxTNEj",
        "outputId": "3ac106c5-e89c-4115-f1ca-6524674718f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzVrwJotgHLW"
      },
      "outputs": [],
      "source": [
        "!pip install Gym[all]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8PPx-gXg1Mz"
      },
      "outputs": [],
      "source": [
        "!pip install ufal.pybox2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYjxLIDW6E_m"
      },
      "outputs": [],
      "source": [
        "from ale_py import ALEInterface, SDL_SUPPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3NG7vjnfIYo"
      },
      "outputs": [],
      "source": [
        "!pip install \"gym[atari, accept-rom-license]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syRny4vnjfb6"
      },
      "outputs": [],
      "source": [
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4mqPi8Gjwux"
      },
      "outputs": [],
      "source": [
        "!pip install gym[atari,accept-rom-license]==0.21.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbbCZ5cT6FCK"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT8Dn0C6jwxe"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade git+https://github.com/openai/gym\n",
        "!pip install autorom\n",
        "#AutoRom\n",
        "!pip install --upgrade gym[atari]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llkACDOTfIay"
      },
      "outputs": [],
      "source": [
        "import ale_py\n",
        "import shimmy\n",
        "\n",
        "import gym\n",
        "import gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJRDnFZ6fO1R"
      },
      "outputs": [],
      "source": [
        "!pip install -U gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0aBX5ucfzoz"
      },
      "outputs": [],
      "source": [
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtyxtWDfH_Zs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8Kgl9KjWNjL"
      },
      "outputs": [],
      "source": [
        "# import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ClipReward(gym.RewardWrapper):\n",
        "    def __init__(self, env, min_reward, max_reward):\n",
        "        super().__init__(env)\n",
        "        self.min_reward = min_reward\n",
        "        self.max_reward = max_reward\n",
        "        self.reward_range = (min_reward, max_reward)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        return np.clip(reward, self.min_reward, self.max_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNMmb5_KWNnj"
      },
      "outputs": [],
      "source": [
        "from gym.spaces import Discrete\n",
        "\n",
        "\n",
        "class DiscreteActions(gym.ActionWrapper):\n",
        "    def __init__(self, env, disc_to_cont):\n",
        "        super().__init__(env)\n",
        "        self.disc_to_cont = disc_to_cont\n",
        "        self.action_space = Discrete(len(disc_to_cont))\n",
        "\n",
        "    def action(self, act):\n",
        "        return self.disc_to_cont[act]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylfgjshOWNrL"
      },
      "outputs": [],
      "source": [
        "class ReacherRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, reward_dist_weight, reward_ctrl_weight):\n",
        "        super().__init__(env)\n",
        "        self.reward_dist_weight = reward_dist_weight\n",
        "        self.reward_ctrl_weight = reward_ctrl_weight\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "        reward = (\n",
        "            self.reward_dist_weight * info[\"reward_dist\"]\n",
        "            + self.reward_ctrl_weight * info[\"reward_ctrl\"]\n",
        "        )\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqspFJdNWNuo"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym.spaces import Box\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RelativePosition(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = Box(shape=(2,), low=-np.inf, high=np.inf)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return obs[\"target\"] - obs[\"agent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Noy4B8YJWNxP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pygame\n",
        "\n",
        "\n",
        "\n",
        "class GridWorldEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
        "\n",
        "    def __init__(self, render_mode=None, size=5):\n",
        "        self.size = size  # The size of the square grid\n",
        "        self.window_size = 512  # The size of the PyGame window\n",
        "\n",
        "        # Observations are dictionaries with the agent's and the target's location.\n",
        "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
        "        self.observation_space = spaces.Dict(\n",
        "            {\n",
        "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
        "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        \"\"\"\n",
        "        The following dictionary maps abstract actions from `self.action_space` to\n",
        "        the direction we will walk in if that action is taken.\n",
        "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
        "        \"\"\"\n",
        "        self._action_to_direction = {\n",
        "            0: np.array([1, 0]),\n",
        "            1: np.array([0, 1]),\n",
        "            2: np.array([-1, 0]),\n",
        "            3: np.array([0, -1]),\n",
        "        }\n",
        "\n",
        "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpS4eyVuXPYI"
      },
      "outputs": [],
      "source": [
        "def _get_obs(self):\n",
        "    return {\"agent\": self._agent_location, \"target\": self._target_location}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ddq7VX2XPhy"
      },
      "outputs": [],
      "source": [
        "def _get_info(self):\n",
        "    return {\n",
        "        \"distance\": np.linalg.norm(\n",
        "            self._agent_location - self._target_location, ord=1\n",
        "        )\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBzrY3JZdMr6"
      },
      "outputs": [],
      "source": [
        "def reset(self, seed=None, options=None):\n",
        "    # We need the following line to seed self.np_random\n",
        "    super().reset(seed=seed)\n",
        "\n",
        "    # Choose the agent's location uniformly at random\n",
        "    self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
        "\n",
        "    # We will sample the target's location randomly until it does not coincide with the agent's location\n",
        "    self._target_location = self._agent_location\n",
        "    while np.array_equal(self._target_location, self._agent_location):\n",
        "        self._target_location = self.np_random.integers(\n",
        "            0, self.size, size=2, dtype=int\n",
        "        )\n",
        "\n",
        "    observation = self._get_obs()\n",
        "    info = self._get_info()\n",
        "\n",
        "    if self.render_mode == \"human\":\n",
        "        self._render_frame()\n",
        "\n",
        "    return observation, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvEAg0tydMuu"
      },
      "outputs": [],
      "source": [
        "def step(self, action):\n",
        "    # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
        "    direction = self._action_to_direction[action]\n",
        "    # We use `np.clip` to make sure we don't leave the grid\n",
        "    self._agent_location = np.clip(\n",
        "        self._agent_location + direction, 0, self.size - 1\n",
        "    )\n",
        "    # An episode is done iff the agent has reached the target\n",
        "    terminated = np.array_equal(self._agent_location, self._target_location)\n",
        "    reward = 1 if terminated else 0  # Binary sparse rewards\n",
        "    observation = self._get_obs()\n",
        "    info = self._get_info()\n",
        "\n",
        "    if self.render_mode == \"human\":\n",
        "        self._render_frame()\n",
        "\n",
        "    return observation, reward, terminated, False, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7bMdc2AdMxs"
      },
      "outputs": [],
      "source": [
        "def render(self):\n",
        "    if self.render_mode == \"rgb_array\":\n",
        "        return self._render_frame()\n",
        "\n",
        "def _render_frame(self):\n",
        "    if self.window is None and self.render_mode == \"human\":\n",
        "        pygame.init()\n",
        "        pygame.display.init()\n",
        "        self.window = pygame.display.set_mode(\n",
        "            (self.window_size, self.window_size)\n",
        "        )\n",
        "    if self.clock is None and self.render_mode == \"human\":\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "    canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "    canvas.fill((255, 255, 255))\n",
        "    pix_square_size = (\n",
        "        self.window_size / self.size\n",
        "    )  # The size of a single grid square in pixels\n",
        "\n",
        "    # First we draw the target\n",
        "    pygame.draw.rect(\n",
        "        canvas,\n",
        "        (255, 0, 0),\n",
        "        pygame.Rect(\n",
        "            pix_square_size * self._target_location,\n",
        "            (pix_square_size, pix_square_size),\n",
        "        ),\n",
        "    )\n",
        "    # Now we draw the agent\n",
        "    pygame.draw.circle(\n",
        "        canvas,\n",
        "        (0, 0, 255),\n",
        "        (self._agent_location + 0.5) * pix_square_size,\n",
        "        pix_square_size / 3,\n",
        "    )\n",
        "\n",
        "    # Finally, add some gridlines\n",
        "    for x in range(self.size + 1):\n",
        "        pygame.draw.line(\n",
        "            canvas,\n",
        "            0,\n",
        "            (0, pix_square_size * x),\n",
        "            (self.window_size, pix_square_size * x),\n",
        "            width=3,\n",
        "        )\n",
        "        pygame.draw.line(\n",
        "            canvas,\n",
        "            0,\n",
        "            (pix_square_size * x, 0),\n",
        "            (pix_square_size * x, self.window_size),\n",
        "            width=3,\n",
        "        )\n",
        "\n",
        "    if self.render_mode == \"human\":\n",
        "        # The following line copies our drawings from `canvas` to the visible window\n",
        "        self.window.blit(canvas, canvas.get_rect())\n",
        "        pygame.event.pump()\n",
        "        pygame.display.update()\n",
        "\n",
        "        # We need to ensure that human-rendering occurs at the predefined framerate.\n",
        "        # The following line will automatically add a delay to keep the framerate stable.\n",
        "        self.clock.tick(self.metadata[\"render_fps\"])\n",
        "    else:  # rgb_array\n",
        "        return np.transpose(\n",
        "            np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQBOPVQCdM1R"
      },
      "outputs": [],
      "source": [
        "def close(self):\n",
        "    if self.window is not None:\n",
        "        pygame.display.quit()\n",
        "        pygame.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r18TtGgVSQHh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07t38IwSR3k2"
      },
      "outputs": [],
      "source": [
        "full_action_space=True\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljBJC5XxSPCh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9JFIFcVXPvK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gym import ActionWrapper, ObservationWrapper, RewardWrapper, Wrapper\n",
        "\n",
        "from gym.spaces import Box, Discrete\n",
        "\n",
        "\n",
        "class RelativePosition(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = Box(shape=(2,), low=-np.inf, high=np.inf)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return obs[\"target\"] - obs[\"agent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK9W8gWxfITg"
      },
      "outputs": [],
      "source": [
        "from typing import SupportsFloat\n",
        "\n",
        "\n",
        "class ClipReward(RewardWrapper):\n",
        "    def __init__(self, env, min_reward, max_reward):\n",
        "        super().__init__(env)\n",
        "        self.min_reward = min_reward\n",
        "        self.max_reward = max_reward\n",
        "        self.reward_range = (min_reward, max_reward)\n",
        "\n",
        "    def reward(self, r: SupportsFloat) -> SupportsFloat:\n",
        "        return np.clip(r, self.min_reward, self.max_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzWo4y8PfIWF"
      },
      "outputs": [],
      "source": [
        "class ReacherRewardWrapper(Wrapper):\n",
        "    def __init__(self, env, reward_dist_weight, reward_ctrl_weight):\n",
        "        super().__init__(env)\n",
        "        self.reward_dist_weight = reward_dist_weight\n",
        "        self.reward_ctrl_weight = reward_ctrl_weight\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "        reward = (\n",
        "            self.reward_dist_weight * info[\"reward_dist\"]\n",
        "            + self.reward_ctrl_weight * info[\"reward_ctrl\"]\n",
        "        )\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMma9hq5fIQl"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "class DiscreteActions(ActionWrapper):\n",
        "    def __init__(self, env, disc_to_cont):\n",
        "        super().__init__(env)\n",
        "        self.disc_to_cont = disc_to_cont\n",
        "        self.action_space = Discrete(len(disc_to_cont))\n",
        "\n",
        "    def action(self, act):\n",
        "        return self.disc_to_cont[act]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gymnasium.make(\"ALE/Freeway-v5\")\n",
        "    wrapped_env = DiscreteActions(\n",
        "        env, [np.array([1, 0]), np.array([-1, 0]), np.array([0, 1]), np.array([0, -1])]\n",
        "    )\n",
        "    print(wrapped_env.action_space)  # Discrete(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM7yur8vHokB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IjjiAo3IdkB"
      },
      "source": [
        "cod for change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu_O5-8NHom0"
      },
      "outputs": [],
      "source": [
        "#cod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWL9J9CwHopo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zFUuSFGEiPq"
      },
      "outputs": [],
      "source": [
        "class ExpirienceReplay(deque):\n",
        "    def sample(self, size):\n",
        "        batch = random.sample(self, size)\n",
        "        return list(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5FA6iqmeCNo"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyS23AxdEiSA"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.steps = 0  # Do not change\n",
        "        self.model = models.resnet34(pretrained=False, num_classes = action_dim).to(DEVICE)  # Torch model\n",
        "        self.target_model = models.resnet34(pretrained=False, num_classes = action_dim).to(DEVICE)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.buffer = ExpirienceReplay(maxlen=BUFFER_SIZE)\n",
        "        self.optimizer = Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
        "        self.criteria = nn.MSELoss()\n",
        "        self.scheduler = lr_scheduler.StepLR(\n",
        "            self.optimizer, step_size=10000, gamma=0.8\n",
        "        )\n",
        "        self.transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "    def consume_transition(self, transition):\n",
        "        state, action, next_state, reward, done = transition\n",
        "        state = self.transforms(state).unsqueeze(0)\n",
        "        next_state = self.transforms(next_state).unsqueeze(0)\n",
        "        transition = state, action, next_state, reward, done\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample_batch(self):\n",
        "        batch = self.buffer.sample(BATCH_SIZE)\n",
        "        state, action, next_state, reward, done = batch\n",
        "        state = torch.cat(state, dim=0)\n",
        "        action = torch.tensor(np.array(action, dtype=np.int64))\n",
        "        next_state = torch.cat(next_state, dim=0)\n",
        "        reward = torch.tensor(np.array(reward, dtype=np.float32))\n",
        "        done = torch.tensor(np.array(done, dtype=np.int32))\n",
        "        return state, action, next_state, reward, done\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        if not self.model.training:\n",
        "            self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        state, action, next_state, reward, done = batch\n",
        "        current_q = self.model(state.to(DEVICE))\n",
        "        next_q = self.model(state.to(DEVICE))\n",
        "        next_action = torch.argmax(next_q, 1)\n",
        "        next_target_q = self.target_model(next_state.to(DEVICE))\n",
        "        action_reward = current_q.gather(1, action.view(-1, 1).to(DEVICE))\n",
        "        next_actions_reward = next_target_q.gather(1, next_action.view(-1, 1))\n",
        "        next_actions_reward = next_actions_reward.squeeze(1) * (1 - done.to(DEVICE))\n",
        "        loss = self.criteria(\n",
        "            action_reward.squeeze(1), reward.to(DEVICE) + GAMMA * next_actions_reward\n",
        "        )\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        if self.steps > 1_000_000:\n",
        "            self.scheduler.step(loss)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def act(self, state, target=False):\n",
        "        full_action_space=True\n",
        "        if self.model.training:\n",
        "            self.model.eval()\n",
        "        network = self.target_model if target else self.model\n",
        "        state = self.transforms(state).to(DEVICE)\n",
        "        action_rewards = network(state.unsqueeze(0)).squeeze(0).detach().cpu().numpy()\n",
        "        return np.argmax(action_rewards)\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.consume_transition(transition)\n",
        "        if self.steps % STEPS_PER_UPDATE == 0:\n",
        "            batch = self.sample_batch()\n",
        "            self.train_step(batch)\n",
        "        if self.steps % STEPS_PER_TARGET_UPDATE == 0:\n",
        "            self.update_target_network()\n",
        "        self.steps += 1\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.model.state_dict(), \"agent.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlYzreeMEiUs"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(agent, episodes=5):\n",
        "    env = gymnasium.make(\"ALE/Freeway-v5\")\n",
        "    returns = []\n",
        "    agent.model.eval()\n",
        "    for _ in range(episodes):\n",
        "        done = False\n",
        "        state = env.reset()[0]\n",
        "        total_reward = 0.0\n",
        "\n",
        "        while not done:\n",
        "            state, reward, done, _, _ = env.step(agent.act(state))\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "    agent.model.train()\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAWmEDm_Xq90"
      },
      "outputs": [],
      "source": [
        "# dqn = DQN(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
        "# torch.load(dqn, 'dqn.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqRMU2jip2gb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9SPe1cxEiXS"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = gymnasium.make(\"ALE/Freeway-v5\")\n",
        "    dqn = DQN(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
        "    eps = 0.1\n",
        "    eps_decay = 2\n",
        "    eps_min = 0.01\n",
        "\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    for _ in range(INITIAL_STEPS):\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        dqn.consume_transition((state, action, next_state, reward, done))\n",
        "\n",
        "        state = next_state if not done else env.reset()\n",
        "\n",
        "    for i in range(TRANSITIONS):\n",
        "\n",
        "        if i % 25_000 == 0:\n",
        "            eps = max(eps / eps_decay, eps_min)\n",
        "\n",
        "        if random.random() < eps:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = dqn.act(state)\n",
        "\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        dqn.update((state, action, next_state, reward, done))\n",
        "\n",
        "        state = next_state if not done else env.reset()[0]\n",
        "\n",
        "        if (i + 1) % (TRANSITIONS // 100) == 0:\n",
        "            rewards = evaluate_policy(dqn, 5)\n",
        "            print(\n",
        "                f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}\"\n",
        "            )\n",
        "            dqn.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z4H8HMGf4YH"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCR7F-NrcH7b"
      },
      "outputs": [],
      "source": [
        "# import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi8iyQNuniBp"
      },
      "outputs": [],
      "source": [
        "torch.save(dqn, 'dqn6.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAmxQMu9yxBW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MPEG')\n",
        "out = cv2.VideoWriter('output6.avi', fourcc, 4.0, (next_state.shape[1], next_state.shape[0]))\n",
        "state, _ = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = dqn.act(state)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    state = next_state\n",
        "    out.write(next_state)\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgikIFy6cMiw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}