{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KseniyaShilova/rl-game/blob/main/rlGameVideo1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TmrznGhOEiEz"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import torch\n",
        "#from gym import make\n",
        "from torch import nn\n",
        "from torch.optim import Adam, lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h9qsCDLlEiHs"
      },
      "outputs": [],
      "source": [
        "GAMMA = 0.99\n",
        "INITIAL_STEPS = 1024\n",
        "TRANSITIONS = 4_444\n",
        "STEPS_PER_UPDATE = 5\n",
        "STEPS_PER_TARGET_UPDATE = STEPS_PER_UPDATE * 1000\n",
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 85_000\n",
        "LEARNING_RATE =  0.000001\n",
        "DEVICE = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBaVU0ZeXrSr",
        "outputId": "58e2ae88-05a3-4b3e-ee45-cafaccccb745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Arcade-Learning-Environment'...\n",
            "remote: Enumerating objects: 9404, done.\u001b[K\n",
            "remote: Counting objects: 100% (1199/1199), done.\u001b[K\n",
            "remote: Compressing objects: 100% (507/507), done.\u001b[K\n",
            "remote: Total 9404 (delta 770), reused 1002 (delta 662), pack-reused 8205\u001b[K\n",
            "Receiving objects: 100% (9404/9404), 7.68 MiB | 23.20 MiB/s, done.\n",
            "Resolving deltas: 100% (7242/7242), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Farama-Foundation/Arcade-Learning-Environment/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VTBvHoMJEiKK"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/Farama-Foundation/gym-examples\n",
        "# cd gym-examples\n",
        "\n",
        "# source .env/bin/activate\n",
        "#!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rptXPM5EWGft"
      },
      "outputs": [],
      "source": [
        "# from gym.envs.registration import register\n",
        "\n",
        "# register(\n",
        "#     id=\"gym_examples/GridWorld-v0\",\n",
        "#     entry_point=\"gym_examples.envs:GridWorldEnv\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIvyWY8efSth",
        "outputId": "cb78969d-d621-47fd-d01f-a738ebcea76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[atari]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.1.0)\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy\n",
            "Successfully installed ale-py-0.8.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[atari]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlIMAeHxTNEj",
        "outputId": "23b0214d-4eb0-418d-e74a-1a87358d3926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2023.7.22)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=5291e48246e4e4f110116e8ff57b936971db56a0f14d56f55f00bdb108c32bbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzVrwJotgHLW",
        "outputId": "27b23c85-b0ec-4ca8-e912-dfb7c0f9d8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Gym[all] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from Gym[all]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from Gym[all]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from Gym[all]) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from Gym[all])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4>=3.1.0 (from Gym[all])\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.* (from Gym[all])\n",
            "  Downloading swig-4.1.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco-py<2.2,>=2.1 (from Gym[all])\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from Gym[all]) (3.7.1)\n",
            "Collecting box2d-py==2.3.5 (from Gym[all])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from Gym[all]) (4.8.0.76)\n",
            "Collecting mujoco==2.2.0 (from Gym[all])\n",
            "  Downloading mujoco-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from Gym[all]) (2.31.6)\n",
            "Collecting ale-py~=0.7.5 (from Gym[all])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest==7.0.1 (from Gym[all])\n",
            "  Downloading pytest-7.0.1-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->Gym[all]) (1.4.0)\n",
            "Collecting glfw (from mujoco==2.2.0->Gym[all])\n",
            "  Downloading glfw-2.6.2-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->Gym[all]) (3.1.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->Gym[all]) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->Gym[all]) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->Gym[all]) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->Gym[all]) (1.3.0)\n",
            "Collecting py>=1.8.2 (from pytest==7.0.1->Gym[all])\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->Gym[all]) (2.0.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->Gym[all]) (6.1.0)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->Gym[all]) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->Gym[all]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->Gym[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->Gym[all]) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->Gym[all]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->Gym[all]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->Gym[all]) (2.8.2)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->Gym[all]) (3.0.5)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->Gym[all]) (1.16.0)\n",
            "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->Gym[all])\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->Gym[all]) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->Gym[all]) (1.16.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install Gym[all]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8PPx-gXg1Mz",
        "outputId": "87be9f40-b271-473e-c55c-ea16ed137a76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ufal.pybox2d\n",
            "  Downloading ufal.pybox2d-2.3.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ufal.pybox2d\n",
            "Successfully installed ufal.pybox2d-2.3.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ufal.pybox2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lYjxLIDW6E_m"
      },
      "outputs": [],
      "source": [
        "from ale_py import ALEInterface, SDL_SUPPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "t3NG7vjnfIYo",
        "outputId": "a19a9e5c-fbd5-40a1-b8e7-b287c31b4f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5 (from gym[accept-rom-license,atari])\n",
            "  Using cached ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (6.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.66.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2023.7.22)\n",
            "Installing collected packages: ale-py\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.8.1\n",
            "    Uninstalling ale-py-0.8.1:\n",
            "      Successfully uninstalled ale-py-0.8.1\n",
            "Successfully installed ale-py-0.7.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ale_py"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \"gym[atari, accept-rom-license]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syRny4vnjfb6",
        "outputId": "d32266b1-ed82-42f3-d132-57182aa4534c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.1.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (4.66.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4mqPi8Gjwux",
        "outputId": "92fb9ccb-b368-4869-c49b-a6cc3c7797ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym[accept-rom-license,atari]==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[atari,accept-rom-license]==0.21.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "DbbCZ5cT6FCK",
        "outputId": "79ec3d58-f23e-439f-cde7-c9835fa21a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.2.1)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari])\n",
            "  Using cached ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.1.0)\n",
            "Installing collected packages: ale-py\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.7.5\n",
            "    Uninstalling ale-py-0.7.5:\n",
            "      Successfully uninstalled ale-py-0.7.5\n",
            "Successfully installed ale-py-0.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ale_py"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT8Dn0C6jwxe",
        "outputId": "3ecb919f-eb06-4957-bd5d-ecbb7c04658c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/gym\n",
            "  Cloning https://github.com/openai/gym to /tmp/pip-req-build-ckws1dxj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/gym /tmp/pip-req-build-ckws1dxj\n",
            "  Resolved https://github.com/openai/gym to commit dcd185843a62953e27c2d54dc8c2d647d604b635\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827619 sha256=86ea05a6f60d730df34f80e4a654db30a7e6ff3fc70df6ce5f0bab8c0ff77613\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5ipmrm2x/wheels/d1/2b/89/4a44b1366d1bc84847d0afaeb7a5cb5b8043d2d73660a49512\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2\n",
            "Requirement already satisfied: autorom in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2023.7.22)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.8.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[atari]) (6.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[atari]) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade git+https://github.com/openai/gym\n",
        "!pip install autorom\n",
        "#AutoRom\n",
        "!pip install --upgrade gym[atari]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "llkACDOTfIay"
      },
      "outputs": [],
      "source": [
        "import ale_py\n",
        "import shimmy\n",
        "\n",
        "import gym\n",
        "import gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJRDnFZ6fO1R",
        "outputId": "1732d4a8-6deb-4849-fe9c-39c7470c5934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0aBX5ucfzoz",
        "outputId": "95c47003-5d74-4e63-8383-87e83ff8da23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.8.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[atari]) (6.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[atari]) (4.5.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (4.66.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VtyxtWDfH_Zs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q8Kgl9KjWNjL"
      },
      "outputs": [],
      "source": [
        "# import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ClipReward(gym.RewardWrapper):\n",
        "    def __init__(self, env, min_reward, max_reward):\n",
        "        super().__init__(env)\n",
        "        self.min_reward = min_reward\n",
        "        self.max_reward = max_reward\n",
        "        self.reward_range = (min_reward, max_reward)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        return np.clip(reward, self.min_reward, self.max_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DNMmb5_KWNnj"
      },
      "outputs": [],
      "source": [
        "from gym.spaces import Discrete\n",
        "\n",
        "\n",
        "class DiscreteActions(gym.ActionWrapper):\n",
        "    def __init__(self, env, disc_to_cont):\n",
        "        super().__init__(env)\n",
        "        self.disc_to_cont = disc_to_cont\n",
        "        self.action_space = Discrete(len(disc_to_cont))\n",
        "\n",
        "    def action(self, act):\n",
        "        return self.disc_to_cont[act]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ylfgjshOWNrL"
      },
      "outputs": [],
      "source": [
        "class ReacherRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env, reward_dist_weight, reward_ctrl_weight):\n",
        "        super().__init__(env)\n",
        "        self.reward_dist_weight = reward_dist_weight\n",
        "        self.reward_ctrl_weight = reward_ctrl_weight\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "        reward = (\n",
        "            self.reward_dist_weight * info[\"reward_dist\"]\n",
        "            + self.reward_ctrl_weight * info[\"reward_ctrl\"]\n",
        "        )\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iqspFJdNWNuo"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym.spaces import Box\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class RelativePosition(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = Box(shape=(2,), low=-np.inf, high=np.inf)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return obs[\"target\"] - obs[\"agent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Noy4B8YJWNxP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pygame\n",
        "\n",
        "\n",
        "\n",
        "class GridWorldEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
        "\n",
        "    def __init__(self, render_mode=None, size=5):\n",
        "        self.size = size  # The size of the square grid\n",
        "        self.window_size = 512  # The size of the PyGame window\n",
        "\n",
        "        # Observations are dictionaries with the agent's and the target's location.\n",
        "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
        "        self.observation_space = spaces.Dict(\n",
        "            {\n",
        "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
        "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        \"\"\"\n",
        "        The following dictionary maps abstract actions from `self.action_space` to\n",
        "        the direction we will walk in if that action is taken.\n",
        "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
        "        \"\"\"\n",
        "        self._action_to_direction = {\n",
        "            0: np.array([1, 0]),\n",
        "            1: np.array([0, 1]),\n",
        "            2: np.array([-1, 0]),\n",
        "            3: np.array([0, -1]),\n",
        "        }\n",
        "\n",
        "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rpS4eyVuXPYI"
      },
      "outputs": [],
      "source": [
        "def _get_obs(self):\n",
        "    return {\"agent\": self._agent_location, \"target\": self._target_location}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5Ddq7VX2XPhy"
      },
      "outputs": [],
      "source": [
        "def _get_info(self):\n",
        "    return {\n",
        "        \"distance\": np.linalg.norm(\n",
        "            self._agent_location - self._target_location, ord=1\n",
        "        )\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mBzrY3JZdMr6"
      },
      "outputs": [],
      "source": [
        "def reset(self, seed=None, options=None):\n",
        "    # We need the following line to seed self.np_random\n",
        "    super().reset(seed=seed)\n",
        "\n",
        "    # Choose the agent's location uniformly at random\n",
        "    self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
        "\n",
        "    # We will sample the target's location randomly until it does not coincide with the agent's location\n",
        "    self._target_location = self._agent_location\n",
        "    while np.array_equal(self._target_location, self._agent_location):\n",
        "        self._target_location = self.np_random.integers(\n",
        "            0, self.size, size=2, dtype=int\n",
        "        )\n",
        "\n",
        "    observation = self._get_obs()\n",
        "    info = self._get_info()\n",
        "\n",
        "    if self.render_mode == \"human\":\n",
        "        self._render_frame()\n",
        "\n",
        "    return observation, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zvEAg0tydMuu"
      },
      "outputs": [],
      "source": [
        "def step(self, action):\n",
        "    # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
        "    direction = self._action_to_direction[action]\n",
        "    # We use `np.clip` to make sure we don't leave the grid\n",
        "    self._agent_location = np.clip(\n",
        "        self._agent_location + direction, 0, self.size - 1\n",
        "    )\n",
        "    # An episode is done iff the agent has reached the target\n",
        "    terminated = np.array_equal(self._agent_location, self._target_location)\n",
        "    reward = 1 if terminated else 0  # Binary sparse rewards\n",
        "    observation = self._get_obs()\n",
        "    info = self._get_info()\n",
        "\n",
        "    if self.render_mode == \"human\":\n",
        "        self._render_frame()\n",
        "\n",
        "    return observation, reward, terminated, False, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "h7bMdc2AdMxs"
      },
      "outputs": [],
      "source": [
        "def render(self):\n",
        "    if self.render_mode == \"rgb_array\":\n",
        "        return self._render_frame()\n",
        "\n",
        "def _render_frame(self):\n",
        "    if self.window is None and self.render_mode == \"human\":\n",
        "        pygame.init()\n",
        "        pygame.display.init()\n",
        "        self.window = pygame.display.set_mode(\n",
        "            (self.window_size, self.window_size)\n",
        "        )\n",
        "    if self.clock is None and self.render_mode == \"human\":\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "    canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "    canvas.fill((255, 255, 255))\n",
        "    pix_square_size = (\n",
        "        self.window_size / self.size\n",
        "    )  # The size of a single grid square in pixels\n",
        "\n",
        "    # First we draw the target\n",
        "    pygame.draw.rect(\n",
        "        canvas,\n",
        "        (255, 0, 0),\n",
        "        pygame.Rect(\n",
        "            pix_square_size * self._target_location,\n",
        "            (pix_square_size, pix_square_size),\n",
        "        ),\n",
        "    )\n",
        "    # Now we draw the agent\n",
        "    pygame.draw.circle(\n",
        "        canvas,\n",
        "        (0, 0, 255),\n",
        "        (self._agent_location + 0.5) * pix_square_size,\n",
        "        pix_square_size / 3,\n",
        "    )\n",
        "\n",
        "    # Finally, add some gridlines\n",
        "    for x in range(self.size + 1):\n",
        "        pygame.draw.line(\n",
        "            canvas,\n",
        "            0,\n",
        "            (0, pix_square_size * x),\n",
        "            (self.window_size, pix_square_size * x),\n",
        "            width=3,\n",
        "        )\n",
        "        pygame.draw.line(\n",
        "            canvas,\n",
        "            0,\n",
        "            (pix_square_size * x, 0),\n",
        "            (pix_square_size * x, self.window_size),\n",
        "            width=3,\n",
        "        )\n",
        "\n",
        "    if self.render_mode == \"human\":\n",
        "        # The following line copies our drawings from `canvas` to the visible window\n",
        "        self.window.blit(canvas, canvas.get_rect())\n",
        "        pygame.event.pump()\n",
        "        pygame.display.update()\n",
        "\n",
        "        # We need to ensure that human-rendering occurs at the predefined framerate.\n",
        "        # The following line will automatically add a delay to keep the framerate stable.\n",
        "        self.clock.tick(self.metadata[\"render_fps\"])\n",
        "    else:  # rgb_array\n",
        "        return np.transpose(\n",
        "            np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FQBOPVQCdM1R"
      },
      "outputs": [],
      "source": [
        "def close(self):\n",
        "    if self.window is not None:\n",
        "        pygame.display.quit()\n",
        "        pygame.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "r18TtGgVSQHh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "07t38IwSR3k2"
      },
      "outputs": [],
      "source": [
        "full_action_space=True\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ljBJC5XxSPCh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Z9JFIFcVXPvK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gym import ActionWrapper, ObservationWrapper, RewardWrapper, Wrapper\n",
        "\n",
        "from gym.spaces import Box, Discrete\n",
        "\n",
        "\n",
        "class RelativePosition(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = Box(shape=(2,), low=-np.inf, high=np.inf)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return obs[\"target\"] - obs[\"agent\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pK9W8gWxfITg"
      },
      "outputs": [],
      "source": [
        "from typing import SupportsFloat\n",
        "\n",
        "\n",
        "class ClipReward(RewardWrapper):\n",
        "    def __init__(self, env, min_reward, max_reward):\n",
        "        super().__init__(env)\n",
        "        self.min_reward = min_reward\n",
        "        self.max_reward = max_reward\n",
        "        self.reward_range = (min_reward, max_reward)\n",
        "\n",
        "    def reward(self, r: SupportsFloat) -> SupportsFloat:\n",
        "        return np.clip(r, self.min_reward, self.max_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bzWo4y8PfIWF"
      },
      "outputs": [],
      "source": [
        "class ReacherRewardWrapper(Wrapper):\n",
        "    def __init__(self, env, reward_dist_weight, reward_ctrl_weight):\n",
        "        super().__init__(env)\n",
        "        self.reward_dist_weight = reward_dist_weight\n",
        "        self.reward_ctrl_weight = reward_ctrl_weight\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "        reward = (\n",
        "            self.reward_dist_weight * info[\"reward_dist\"]\n",
        "            + self.reward_ctrl_weight * info[\"reward_ctrl\"]\n",
        "        )\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMma9hq5fIQl",
        "outputId": "fdab3679-1a56-4f8f-958e-3cfb7dc143ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "class DiscreteActions(ActionWrapper):\n",
        "    def __init__(self, env, disc_to_cont):\n",
        "        super().__init__(env)\n",
        "        self.disc_to_cont = disc_to_cont\n",
        "        self.action_space = Discrete(len(disc_to_cont))\n",
        "\n",
        "    def action(self, act):\n",
        "        return self.disc_to_cont[act]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gymnasium.make(\"ALE/Freeway-v5\")\n",
        "    wrapped_env = DiscreteActions(\n",
        "        env, [np.array([1, 0]), np.array([-1, 0]), np.array([0, 1]), np.array([0, -1])]\n",
        "    )\n",
        "    print(wrapped_env.action_space)  # Discrete(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "FM7yur8vHokB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IjjiAo3IdkB"
      },
      "source": [
        "cod for change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Hu_O5-8NHom0"
      },
      "outputs": [],
      "source": [
        "#cod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "mWL9J9CwHopo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9zFUuSFGEiPq"
      },
      "outputs": [],
      "source": [
        "class ExpirienceReplay(deque):\n",
        "    def sample(self, size):\n",
        "        batch = random.sample(self, size)\n",
        "        return list(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "K5FA6iqmeCNo"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WyS23AxdEiSA"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.steps = 0  # Do not change\n",
        "        self.model = models.resnet18(pretrained=False, num_classes = action_dim).to(DEVICE)  # Torch model\n",
        "        self.target_model = models.resnet18(pretrained=False, num_classes = action_dim).to(DEVICE)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.buffer = ExpirienceReplay(maxlen=BUFFER_SIZE)\n",
        "        self.optimizer = Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
        "        self.criteria = nn.MSELoss()\n",
        "        self.scheduler = lr_scheduler.StepLR(\n",
        "            self.optimizer, step_size=10000, gamma=0.8\n",
        "        )\n",
        "        self.transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "    def consume_transition(self, transition):\n",
        "        state, action, next_state, reward, done = transition\n",
        "        state = self.transforms(state).unsqueeze(0)\n",
        "        next_state = self.transforms(next_state).unsqueeze(0)\n",
        "        transition = state, action, next_state, reward, done\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample_batch(self):\n",
        "        batch = self.buffer.sample(BATCH_SIZE)\n",
        "        state, action, next_state, reward, done = batch\n",
        "        state = torch.cat(state, dim=0)\n",
        "        action = torch.tensor(np.array(action, dtype=np.int64))\n",
        "        next_state = torch.cat(next_state, dim=0)\n",
        "        reward = torch.tensor(np.array(reward, dtype=np.float32))\n",
        "        done = torch.tensor(np.array(done, dtype=np.int32))\n",
        "        return state, action, next_state, reward, done\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        if not self.model.training:\n",
        "            self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        state, action, next_state, reward, done = batch\n",
        "        current_q = self.model(state.to(DEVICE))\n",
        "        next_q = self.model(state.to(DEVICE))\n",
        "        next_action = torch.argmax(next_q, 1)\n",
        "        next_target_q = self.target_model(next_state.to(DEVICE))\n",
        "        action_reward = current_q.gather(1, action.view(-1, 1).to(DEVICE))\n",
        "        next_actions_reward = next_target_q.gather(1, next_action.view(-1, 1))\n",
        "        next_actions_reward = next_actions_reward.squeeze(1) * (1 - done.to(DEVICE))\n",
        "        loss = self.criteria(\n",
        "            action_reward.squeeze(1), reward.to(DEVICE) + GAMMA * next_actions_reward\n",
        "        )\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        if self.steps > 1_000_000:\n",
        "            self.scheduler.step(loss)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def act(self, state, target=False):\n",
        "        full_action_space=True\n",
        "        if self.model.training:\n",
        "            self.model.eval()\n",
        "        network = self.target_model if target else self.model\n",
        "        state = self.transforms(state).to(DEVICE)\n",
        "        action_rewards = network(state.unsqueeze(0)).squeeze(0).detach().cpu().numpy()\n",
        "        return np.argmax(action_rewards)\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.consume_transition(transition)\n",
        "        if self.steps % STEPS_PER_UPDATE == 0:\n",
        "            batch = self.sample_batch()\n",
        "            self.train_step(batch)\n",
        "        if self.steps % STEPS_PER_TARGET_UPDATE == 0:\n",
        "            self.update_target_network()\n",
        "        self.steps += 1\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.model.state_dict(), \"agent.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "WlYzreeMEiUs"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(agent, episodes=5):\n",
        "    env = gymnasium.make(\"ALE/Freeway-v5\")\n",
        "    returns = []\n",
        "    agent.model.eval()\n",
        "    for _ in range(episodes):\n",
        "        done = False\n",
        "        state = env.reset()[0]\n",
        "        total_reward = 0.0\n",
        "\n",
        "        while not done:\n",
        "            state, reward, done, _, _ = env.step(agent.act(state))\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "    agent.model.train()\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "zAWmEDm_Xq90"
      },
      "outputs": [],
      "source": [
        "# dqn = DQN(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
        "# torch.load(dqn, 'dqn.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aqRMU2jip2gb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9SPe1cxEiXS",
        "outputId": "c4977e39-45a7-4cc1-fc76-5a6d0779838b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 44, Reward mean: 21.4, Reward std: 0.7999999999999999\n",
            "Step: 88, Reward mean: 22.8, Reward std: 0.9797958971132713\n",
            "Step: 132, Reward mean: 18.4, Reward std: 0.7999999999999999\n",
            "Step: 176, Reward mean: 14.8, Reward std: 0.39999999999999997\n",
            "Step: 220, Reward mean: 10.6, Reward std: 2.0591260281974\n",
            "Step: 264, Reward mean: 6.2, Reward std: 1.32664991614216\n",
            "Step: 308, Reward mean: 1.8, Reward std: 0.9797958971132712\n",
            "Step: 352, Reward mean: 0.6, Reward std: 0.48989794855663565\n",
            "Step: 396, Reward mean: 2.0, Reward std: 1.0954451150103321\n",
            "Step: 440, Reward mean: 2.4, Reward std: 1.624807680927192\n",
            "Step: 484, Reward mean: 4.2, Reward std: 2.1354156504062622\n",
            "Step: 528, Reward mean: 7.4, Reward std: 1.9595917942265424\n",
            "Step: 572, Reward mean: 9.0, Reward std: 1.6733200530681511\n",
            "Step: 616, Reward mean: 10.0, Reward std: 1.4142135623730951\n",
            "Step: 660, Reward mean: 10.0, Reward std: 2.1908902300206643\n",
            "Step: 704, Reward mean: 10.8, Reward std: 1.7204650534085253\n",
            "Step: 748, Reward mean: 11.0, Reward std: 0.6324555320336759\n",
            "Step: 792, Reward mean: 12.2, Reward std: 1.7204650534085253\n",
            "Step: 836, Reward mean: 12.2, Reward std: 1.6\n",
            "Step: 880, Reward mean: 11.4, Reward std: 1.019803902718557\n",
            "Step: 924, Reward mean: 11.8, Reward std: 0.9797958971132713\n",
            "Step: 968, Reward mean: 11.6, Reward std: 0.7999999999999999\n",
            "Step: 1012, Reward mean: 11.8, Reward std: 0.39999999999999997\n",
            "Step: 1056, Reward mean: 13.2, Reward std: 1.4696938456699067\n",
            "Step: 1100, Reward mean: 13.0, Reward std: 1.0954451150103321\n",
            "Step: 1144, Reward mean: 12.6, Reward std: 0.48989794855663565\n",
            "Step: 1188, Reward mean: 13.0, Reward std: 1.0954451150103321\n",
            "Step: 1232, Reward mean: 13.4, Reward std: 2.0591260281974\n",
            "Step: 1276, Reward mean: 15.4, Reward std: 1.8547236990991407\n",
            "Step: 1320, Reward mean: 15.4, Reward std: 1.2\n",
            "Step: 1364, Reward mean: 14.0, Reward std: 1.0954451150103321\n",
            "Step: 1408, Reward mean: 14.4, Reward std: 2.1540659228538015\n",
            "Step: 1452, Reward mean: 15.8, Reward std: 0.9797958971132713\n",
            "Step: 1496, Reward mean: 17.4, Reward std: 2.0591260281974\n",
            "Step: 1540, Reward mean: 14.6, Reward std: 1.2\n",
            "Step: 1584, Reward mean: 14.8, Reward std: 1.4696938456699067\n",
            "Step: 1628, Reward mean: 16.8, Reward std: 1.9390719429665317\n",
            "Step: 1672, Reward mean: 16.0, Reward std: 1.4142135623730951\n",
            "Step: 1716, Reward mean: 15.2, Reward std: 1.9390719429665317\n",
            "Step: 1760, Reward mean: 16.6, Reward std: 1.3564659966250536\n",
            "Step: 1804, Reward mean: 16.2, Reward std: 1.1661903789690602\n",
            "Step: 1848, Reward mean: 14.4, Reward std: 0.4898979485566356\n",
            "Step: 1892, Reward mean: 14.4, Reward std: 1.4966629547095764\n",
            "Step: 1936, Reward mean: 17.2, Reward std: 1.4696938456699067\n",
            "Step: 1980, Reward mean: 17.2, Reward std: 0.7483314773547882\n",
            "Step: 2024, Reward mean: 17.0, Reward std: 1.0954451150103321\n",
            "Step: 2068, Reward mean: 16.6, Reward std: 1.019803902718557\n",
            "Step: 2112, Reward mean: 16.0, Reward std: 1.8973665961010275\n",
            "Step: 2156, Reward mean: 16.6, Reward std: 1.4966629547095764\n",
            "Step: 2200, Reward mean: 16.2, Reward std: 0.9797958971132713\n",
            "Step: 2244, Reward mean: 16.8, Reward std: 0.9797958971132713\n",
            "Step: 2288, Reward mean: 19.0, Reward std: 1.0954451150103321\n",
            "Step: 2332, Reward mean: 18.6, Reward std: 0.4898979485566356\n",
            "Step: 2376, Reward mean: 18.2, Reward std: 0.9797958971132712\n",
            "Step: 2420, Reward mean: 17.0, Reward std: 0.6324555320336759\n",
            "Step: 2464, Reward mean: 17.4, Reward std: 1.854723699099141\n",
            "Step: 2508, Reward mean: 17.2, Reward std: 0.7483314773547882\n",
            "Step: 2552, Reward mean: 17.4, Reward std: 0.8\n",
            "Step: 2596, Reward mean: 16.0, Reward std: 1.7888543819998317\n",
            "Step: 2640, Reward mean: 16.0, Reward std: 1.8973665961010275\n",
            "Step: 2684, Reward mean: 17.2, Reward std: 0.4\n",
            "Step: 2728, Reward mean: 17.4, Reward std: 0.7999999999999999\n",
            "Step: 2772, Reward mean: 17.6, Reward std: 1.3564659966250536\n",
            "Step: 2816, Reward mean: 16.2, Reward std: 0.39999999999999997\n",
            "Step: 2860, Reward mean: 17.4, Reward std: 1.0198039027185568\n",
            "Step: 2904, Reward mean: 18.0, Reward std: 1.8973665961010275\n",
            "Step: 2948, Reward mean: 17.8, Reward std: 0.39999999999999997\n",
            "Step: 2992, Reward mean: 18.2, Reward std: 0.7483314773547883\n",
            "Step: 3036, Reward mean: 17.4, Reward std: 1.3564659966250538\n",
            "Step: 3080, Reward mean: 18.0, Reward std: 1.4142135623730951\n",
            "Step: 3124, Reward mean: 17.4, Reward std: 1.0198039027185568\n",
            "Step: 3168, Reward mean: 17.4, Reward std: 0.4898979485566356\n",
            "Step: 3212, Reward mean: 16.8, Reward std: 1.6\n",
            "Step: 3256, Reward mean: 17.4, Reward std: 0.4898979485566356\n",
            "Step: 3300, Reward mean: 16.6, Reward std: 1.019803902718557\n",
            "Step: 3344, Reward mean: 18.4, Reward std: 0.7999999999999999\n",
            "Step: 3388, Reward mean: 17.2, Reward std: 1.32664991614216\n",
            "Step: 3432, Reward mean: 17.4, Reward std: 1.8547236990991407\n",
            "Step: 3476, Reward mean: 17.8, Reward std: 2.039607805437114\n",
            "Step: 3520, Reward mean: 19.2, Reward std: 1.32664991614216\n",
            "Step: 3564, Reward mean: 18.2, Reward std: 3.059411708155671\n",
            "Step: 3608, Reward mean: 17.4, Reward std: 1.4966629547095764\n",
            "Step: 3652, Reward mean: 17.8, Reward std: 1.1661903789690602\n",
            "Step: 3696, Reward mean: 16.4, Reward std: 0.4898979485566356\n",
            "Step: 3740, Reward mean: 18.0, Reward std: 1.0954451150103321\n",
            "Step: 3784, Reward mean: 19.0, Reward std: 1.4142135623730951\n",
            "Step: 3828, Reward mean: 18.6, Reward std: 1.4966629547095764\n",
            "Step: 3872, Reward mean: 18.0, Reward std: 0.6324555320336759\n",
            "Step: 3916, Reward mean: 17.0, Reward std: 1.0954451150103321\n",
            "Step: 3960, Reward mean: 18.2, Reward std: 2.0396078054371136\n",
            "Step: 4004, Reward mean: 18.2, Reward std: 1.1661903789690602\n",
            "Step: 4048, Reward mean: 18.8, Reward std: 1.5999999999999999\n",
            "Step: 4092, Reward mean: 17.2, Reward std: 1.6\n",
            "Step: 4136, Reward mean: 18.8, Reward std: 1.5999999999999999\n",
            "Step: 4180, Reward mean: 19.6, Reward std: 1.019803902718557\n",
            "Step: 4224, Reward mean: 17.4, Reward std: 1.7435595774162693\n",
            "Step: 4268, Reward mean: 17.2, Reward std: 0.9797958971132713\n",
            "Step: 4312, Reward mean: 17.4, Reward std: 0.4898979485566356\n",
            "Step: 4356, Reward mean: 17.6, Reward std: 1.019803902718557\n",
            "Step: 4400, Reward mean: 19.4, Reward std: 0.8\n",
            "Step: 4444, Reward mean: 18.0, Reward std: 0.6324555320336759\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = gymnasium.make(\"ALE/Freeway-v5\")\n",
        "    dqn = DQN(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
        "    eps = 0.1\n",
        "    eps_decay = 2\n",
        "    eps_min = 0.01\n",
        "\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    for _ in range(INITIAL_STEPS):\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        dqn.consume_transition((state, action, next_state, reward, done))\n",
        "\n",
        "        state = next_state if not done else env.reset()\n",
        "\n",
        "    for i in range(TRANSITIONS):\n",
        "\n",
        "        if i % 25_000 == 0:\n",
        "            eps = max(eps / eps_decay, eps_min)\n",
        "\n",
        "        if random.random() < eps:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = dqn.act(state)\n",
        "\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        dqn.update((state, action, next_state, reward, done))\n",
        "\n",
        "        state = next_state if not done else env.reset()[0]\n",
        "\n",
        "        if (i + 1) % (TRANSITIONS // 100) == 0:\n",
        "            rewards = evaluate_policy(dqn, 5)\n",
        "            print(\n",
        "                f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}\"\n",
        "            )\n",
        "            dqn.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_z4H8HMGf4YH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6790ee-65d5-4c32-d00b-1ba526316b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Pi8iyQNuniBp"
      },
      "outputs": [],
      "source": [
        "torch.save(dqn, 'dqn.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "iAmxQMu9yxBW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MPEG')\n",
        "out = cv2.VideoWriter('output1.avi', fourcc, 4.0, (next_state.shape[1], next_state.shape[0]))\n",
        "state, _ = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = dqn.act(state)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    state = next_state\n",
        "    out.write(next_state)\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BmsVXi8sNPEt"
      },
      "execution_count": 44,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}